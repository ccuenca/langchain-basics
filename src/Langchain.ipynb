{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e5d20ad-fc15-45e6-b88c-864104fd9eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡haha! Aquí tienes uno:\n",
      "\n",
      "¿Por qué la computadora fue al psicólogo?\n",
      "\n",
      "¡Porque tenía una \"crisis de bytes\"!\n",
      "\n",
      "¿Te gustó?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\", temperature=0, system=\"tu eres un buen bot\")\n",
    "\n",
    "print(llm.invoke(\"dime un chiste\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ab16822-0197-49f8-9d07-8390535a9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡haha! Okay, here\\'s another one:\\n\\n¿Por qué el bot es como una computadora? ¡Porque siempre está procesando información y nunca se \"desactualiza\"!\\n\\n¿Te gustó?']\n"
     ]
    }
   ],
   "source": [
    "print(llm.batch([\"dime otro mas chiste\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11989680-d1f7-4cf0-8841-f181ada96232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'s\n",
      " one\n",
      ":\n",
      "\n",
      "\n",
      "Why\n",
      " couldn\n",
      "'t\n",
      " the\n",
      " bicycle\n",
      " stand\n",
      " up\n",
      " by\n",
      " itself\n",
      "?\n",
      "\n",
      "\n",
      "(W\n",
      "ait\n",
      " for\n",
      " it\n",
      "...)\n",
      "\n",
      "\n",
      "Because\n",
      " it\n",
      " was\n",
      " two\n",
      "-t\n",
      "ired\n",
      "!\n",
      "\n",
      "\n",
      "Hope\n",
      " that\n",
      " made\n",
      " you\n",
      " smile\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Tell me a joke!\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa1eda6c-5565-45b0-959d-e11b3c90ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Hope that made you smile!\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke([{\"content\":\"Tell me a joke\", \"type\": \"user\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcc4ffe1-827a-4cf5-b2c5-bb4eef9c9f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh! Do you want to hear another?\" response_metadata={'model': 'llama3', 'created_at': '2024-05-19T04:54:22.35575Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1536936708, 'load_duration': 2336791, 'prompt_eval_count': 13, 'prompt_eval_duration': 448443000, 'eval_count': 35, 'eval_duration': 1082474000} id='run-9c6beff8-be3c-4061-b8fc-9d12ae395297-0'\n"
     ]
    }
   ],
   "source": [
    "#using \"chat_models\" instead \"llms\"\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "chat_llm = ChatOllama(model=\"llama3\")\n",
    "print(chat_llm.invoke(\"tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64334a3c-fcc9-4576-8ace-f772f71583eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\": \"foo\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "print(BaseMessage(content=\"foo\", type=\"human\").json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49369dbb-fb10-4dc7-b06b-87773f8d4aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(HumanMessage(content=\"Hello There\").type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1286e51-ab80-4d8b-9881-2cfbbe52a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are an expert in kite surfin, and your name is Steve'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an expert in {subject}, and your name is {name}\"\n",
    ")\n",
    "\n",
    "system_prompt = system_prompt_template.format(\n",
    "    subject=\"kite surfin\",\n",
    "    name=\"Steve\"\n",
    ")\n",
    "\n",
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f763c350-4f66-4b92-95e9-8aec67a7b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an helpful AI and your name is Bob\n",
      "Human: Hello, how are you doing?\n",
      "AI: I´m doing well, thanks!\n",
      "Human: Whats is your name?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an helpful AI and your name is {name}\"\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    system_prompt_template,\n",
    "    HumanMessage(\"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I´m doing well, thanks!\"),\n",
    "    \"{user_input}\"\n",
    "])\n",
    "\n",
    "prompt_value = template.format(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"Whats is your name?\"\n",
    ")\n",
    "\n",
    "print(prompt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbb5e1e5-e211-4cc4-98b9-39c8dbb000c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are an helpful AI and your name is Steve'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content='I´m doing well, thanks!'), HumanMessage(content='What is your name?')]\n"
     ]
    }
   ],
   "source": [
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Steve\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1da83d88-6945-4843-b6c2-cc4970d24a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say hello\n",
      "Say hello\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Instantiation using from_template (recommended)\n",
    "prompt = PromptTemplate.from_template(\"Say {foo}\") \n",
    "print (prompt.format(foo=\"hello\")) # or invoke - it's a Runnable\n",
    "\n",
    "prompt = \"Say {foo}\"\n",
    "print(prompt.format(foo=\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e723fffb-ee6f-4ea1-a078-f5b9ddfa5b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an helpful AI and an expert in LLMs. You give short answers.\n",
      "Human: What is Langchain?\n"
     ]
    }
   ],
   "source": [
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an helpful AI and an expert in {subject}. You give short answers.\"\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    system_prompt_template,    \n",
    "    (\"human\", \"{user_input}\")    \n",
    "])\n",
    "\n",
    "prompt_value = template.format(\n",
    "    subject=\"LLMs\",\n",
    "    user_input=\"What is Langchain?\"\n",
    ")\n",
    "\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9a6e9a7-5a90-4561-b0c3-02e2b05c4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"LangChain is a lightweight, open-source language model that uses a chain of transformer-based models to generate coherent text. It's designed for efficiency and scalability, making it suitable for large-scale natural language processing tasks.\" response_metadata={'model': 'llama3', 'created_at': '2024-05-19T05:29:14.375212Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1504595916, 'load_duration': 3119416, 'prompt_eval_duration': 151753000, 'eval_count': 43, 'eval_duration': 1345905000} id='run-4f9ea358-45ff-4fb2-8fe4-a8034b2aeedf-0'\n"
     ]
    }
   ],
   "source": [
    "chat_llm = ChatOllama(model=\"llama3\")\n",
    "print(chat_llm.invoke(prompt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64097f13-78b5-4c1e-8c88-f36dec69f16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In natural language processing (NLP) and machine learning, embeddings refer to a way of representing words or tokens as numerical vectors that capture their semantic meaning. These vector representations aim to preserve the linguistic context and relationships between words, enabling machines to understand word meanings and analogies.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = template | chat_llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\n",
    "    \"subject\": \"LLMs\",\n",
    "    \"user_input\": \"What is Embeddings?\"\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5cb4ea-4e26-4f65-854e-179e628ded2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
